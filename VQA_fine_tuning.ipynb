{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd996af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50,ResNet50_Weights\n",
    "from transformers import BertModel, BertTokenizer, BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pour charger un dataset VQA, le prétraiter et configurer des DataLoaders pour l'entraînement et la validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea06cef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset_train length: 4\n",
      "Dataset_val length: 2\n",
      "Number of batches in dataloader: 1\n",
      "Number of batches in dataloader: 1\n",
      "Image Tensor Shape: torch.Size([4, 3, 512, 512])\n",
      "Question: Is her back showing?\n",
      "Answer: yes\n",
      "*/*/*/*/*/*/*/*//**/*/*/*/*/*//*///*\n",
      "Image Tensor Shape: torch.Size([2, 3, 512, 512])\n",
      "Question: Is she sitting inside?\n",
      "Answer: yes\n"
     ]
    }
   ],
   "source": [
    "# Configuration du modele sur CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#classer le dataset en #[image,qst,answer]\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataset_path, transform=None):\n",
    "        with open(dataset_path, 'rb') as file:\n",
    "            self.data = pickle.load(file)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def image_to_matrix(self, image_path):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, question, answer = self.data[idx]\n",
    "        image_matrix = self.image_to_matrix(image_path)\n",
    "        #renvoyer la liste qui contient [image,qst,answer]\n",
    "        return image_matrix, question, answer\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),   #la taille attendue par resnet50\n",
    "    transforms.ToTensor(),         \n",
    "])\n",
    "\n",
    "dataset_train_path = \"C:/Users/ACER/OneDrive/Bureau/dataset_v3/vqa_dataset_full.pkl\"\n",
    "vqa_dataset_train = VQADataset(dataset_train_path, transform=transform)\n",
    "train_size = int(0.00001 * len(vqa_dataset_train))\n",
    "rest_size = len(vqa_dataset_train) - train_size\n",
    "#tester le fine tuning du modele avec une petite partie du train_data\n",
    "vqa_dataset_train_10, _ = random_split(vqa_dataset_train, [train_size, rest_size]) \n",
    "\n",
    "\n",
    "\n",
    "dataset_val_path = \"C:/Users/ACER/OneDrive/Bureau/dataset_v3/vqa_dataset_val_full.pkl\"\n",
    "vqa_dataset_val = VQADataset(dataset_val_path, transform=transform)\n",
    "val_size = int(0.00001 * len(vqa_dataset_val))\n",
    "rest_size = len(vqa_dataset_val) - val_size\n",
    "#pour tester le fine tuning du modele avec une petite partie du val data\n",
    "vqa_dataset_val_10, _ = random_split(vqa_dataset_val, [val_size, rest_size])\n",
    "\n",
    "\n",
    "print(f\"Dataset_train length: {len(vqa_dataset_train_10)}\")\n",
    "print(f\"Dataset_val length: {len(vqa_dataset_val_10)}\")\n",
    "\n",
    "\n",
    "\n",
    "#un batch size de 8\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(vqa_dataset_train_10, batch_size=batch_size, shuffle=True) #melanger les donnees a chq epoch\n",
    "val_dataloader = DataLoader(vqa_dataset_val_10, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_batches_train = len(train_dataloader)\n",
    "print(f\"Number of batches in dataloader: {num_batches_train}\")\n",
    "\n",
    "\n",
    "num_batches_val = len(val_dataloader)\n",
    "print(f\"Number of batches in dataloader: {num_batches_val}\")\n",
    "\n",
    "#afficher un element de chaque data\n",
    "\n",
    "for images, questions, answers in train_dataloader:\n",
    "    print(f\"Image Tensor Shape: {images.shape}\")  # Afficher la forme du tensor de l'imag\n",
    "    print(f\"Question: {questions[0]}\")\n",
    "    print(f\"Answer: {answers[0]}\")\n",
    "    \n",
    "    image_to_show = images[0].permute(1, 2, 0).numpy()  # Convertir de [C, H, W] à [H, W, C]\n",
    "    image_to_show = (image_to_show * 255).astype(np.uint8)  # Remettre à une échelle 0-255\n",
    "    Image.fromarray(image_to_show).show()  \n",
    "    break  \n",
    "\n",
    "print(\"*/*/*/*/*/*/*/*//**/*/*/*/*/*//*///*\")\n",
    "    \n",
    "for images, questions, answers in val_dataloader:\n",
    "    print(f\"Image Tensor Shape: {images.shape}\")  \n",
    "    print(f\"Question: {questions[0]}\")\n",
    "    print(f\"Answer: {answers[0]}\")\n",
    "    \n",
    "    image_to_show = images[0].permute(1, 2, 0).numpy()  # Convertir de [C, H, W] à [H, W, C]\n",
    "    image_to_show = (image_to_show * 255).astype(np.uint8)  \n",
    "    Image.fromarray(image_to_show).show()  \n",
    "    break  \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69402e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visual Feature Extractor avec ResNet50\n",
    "class VisualFeatureExtractor(nn.Module):\n",
    "    def __init__(self, hidden_dim=768): #768 est le dim_embed de bert\n",
    "        super(VisualFeatureExtractor, self).__init__()\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT).to(device) #utilisant les poids de resnet pre entraines\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, hidden_dim) #projection sur hidden_dim\n",
    "\n",
    "    def forward(self, image):\n",
    "        image = image.to(device) # l image vers gpu\n",
    "        features = self.resnet(image) #map features\n",
    "        return features  #shape (batch_size,768)\n",
    "\n",
    "# Question Encoder avec BERT\n",
    "class QuestionEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuestionEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def forward(self, question):\n",
    "        inputs = self.tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True) #tokenisation de la qst\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()} #passer tout en gpu\n",
    "        outputs = self.bert(**inputs) #passer les valeurs du dic en param a bert\n",
    "        return outputs.last_hidden_state #shape (batch_size,seq_lg,768)\n",
    "\n",
    "# Cross-Attention \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=768):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=4).to(device) #cross attention avec 4 heads\n",
    "\n",
    "    def forward(self, question_embeddings, visual_features):\n",
    "        visual_features = visual_features.unsqueeze(0) #shape (1,batch_size,768)\n",
    "        question_embeddings = question_embeddings.permute(1, 0, 2)\n",
    "        fusion_features, _ = self.cross_attention(question_embeddings, visual_features, visual_features)\n",
    "        return fusion_features.permute(1, 0, 2) #(batch_size,seq_lg,768)\n",
    "\n",
    "# Answer Generator avec BART\n",
    "class AnswerGenerator(nn.Module):\n",
    "    def __init__(self, hidden_dim=768):\n",
    "        super(AnswerGenerator, self).__init__()\n",
    "        self.bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "        self.fc = nn.Linear(hidden_dim, self.bart.config.d_model)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)  # ajouter normalization pour reduire les fluctuations des valeurs\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            self.bart.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def forward(self, fusion_features, max_length=50):\n",
    "        # normalisation\n",
    "        normalized_features = self.layer_norm(fusion_features)\n",
    "        \n",
    "        # projeter sur la dim attendue par BART\n",
    "        projected_features = self.fc(normalized_features)\n",
    "        \n",
    "        #moyenne sur seq_lg\n",
    "        averaged_features = projected_features.mean(dim=1)\n",
    "        \n",
    "        # Generation de la réponse\n",
    "        outputs = self.bart.generate(\n",
    "            inputs_embeds=averaged_features.unsqueeze(1),\n",
    "            max_length=max_length,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# combiner les sous modèles\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=768):\n",
    "        super(VQAModel, self).__init__()\n",
    "        self.visual_extractor = VisualFeatureExtractor(hidden_dim)\n",
    "        self.question_encoder = QuestionEncoder()\n",
    "        self.cross_attention = CrossAttention(hidden_dim)\n",
    "        self.answer_generator = AnswerGenerator()\n",
    "\n",
    "    def forward(self, image, question):\n",
    "        visual_features = self.visual_extractor(image)\n",
    "        question_embeddings = self.question_encoder(question).to(device)\n",
    "        fusion_features = self.cross_attention(question_embeddings, visual_features).to(device)\n",
    "        answer = self.answer_generator(fusion_features)\n",
    "        return answer\n",
    "\n",
    "# initialiser le modèle\n",
    "model = VQAModel()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8128e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44cf8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning (en premier c'etait pour corss_attention+answer_generator mais ça prend bcp de temps)\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': model.answer_generator.parameters(), 'lr': 5e-5}  # Fine-tune uniquement BART\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "# fonction Fine-tuning \n",
    "def fine_tune_vqa(model, train_dataloader, val_dataloader, num_epochs=5, checkpoint_path=\"C:/Users/ACER/OneDrive/Bureau/dataset_v3/best_model_vqa\"):\n",
    "    best_val_loss = float('inf')\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "    #entrainnement\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Training starts...\\n\")\n",
    "        for batch_idx, (image, question, answer) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad() #initialiser l optimizer\n",
    "            #passer tous les elements en device\n",
    "            image = image.to(device)\n",
    "            question_embeddings = model.question_encoder(question).to(device)\n",
    "            visual_features = model.visual_extractor(image.to(device)).to(device)\n",
    "            fusion_features = model.cross_attention(question_embeddings, visual_features)\n",
    "            outputs = model.answer_generator(fusion_features)\n",
    "            targets = tokenizer(answer, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "            #calculer la difference entre answer_gen et ground_truth\n",
    "            loss = model.answer_generator.bart(input_ids=targets, labels=targets).loss\n",
    "            #ignorer les NaN dans le calcul de la loss totale\n",
    "            if loss is not None and not torch.isnan(loss):\n",
    "                total_train_loss += loss.item()\n",
    "            #backpropag\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            progress = (batch_idx + 1) / len(train_dataloader) * 100\n",
    "            print(f\"Batch {batch_idx}/{len(train_dataloader)} - Loss: {loss.item()} {progress:.2f}%\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Avg training loss: {avg_train_loss}\")\n",
    "\n",
    "        \n",
    "        #validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (image, question, answer) in enumerate(val_dataloader):\n",
    "                image = image.to(device)\n",
    "                question_embeddings = model.question_encoder(question).to(device)\n",
    "                visual_features = model.visual_extractor(image.to(device))\n",
    "                fusion_features = model.cross_attention(question_embeddings, visual_features)\n",
    "                outputs = model.answer_generator(fusion_features)\n",
    "                targets = tokenizer(answer, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "                loss = model.answer_generator.bart(input_ids=targets, labels=targets).loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Validation loss: {avg_val_loss}\")\n",
    "        #sauvegarder le meilleure modèle\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Model saved with val loss: {best_val_loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de672284",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_vqa(model, train_dataloader, val_dataloader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d1e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_model(image_path, question, checkpoint_path):\n",
    "    # Charger le modèle \n",
    "    model = VQAModel().to(device)\n",
    "    #charger les poids_a_jour\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Prétraitement de l'image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Ajouter une dimension batch\n",
    "\n",
    "    # Tokenizer pour la question\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "    # Encoder la question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    question_embeddings = model.question_encoder(question).to(device)\n",
    "\n",
    "    # Extraire les caractéristiques visuelles de l'image\n",
    "    visual_features = model.visual_extractor(image).to(device)\n",
    "\n",
    "    fusion_features = model.cross_attention(question_embeddings, visual_features)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.answer_generator(fusion_features)\n",
    "\n",
    "    # Décoder la réponse\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ee6ee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: This is not a good time to start. Let's start at the beginning. This is a very good time. I'm going to start with the back of the head.This is a great time to stop and look around. This\n"
     ]
    }
   ],
   "source": [
    "image_path = \"C:/Users/ACER/OneDrive/Images/01.jpg\"\n",
    "question = \"how many animal in the image?\"\n",
    "checkpoint_path = \"C:/Users/ACER/Downloads/VQA_epoch2.pth\"\n",
    "#afficher la rép générée (dans cet expl c était sans poids a jour)\n",
    "answer = load_and_evaluate_model(image_path, question, checkpoint_path)\n",
    "print(f\"Answer: {answer}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0b3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
