{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efad2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50,ResNet50_Weights\n",
    "from transformers import BertModel, BertTokenizer, BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7221f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Préparation du dataset d'entrainnement en un seul fichier pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d206f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir les chemins vers les images et les fichiers JSON\n",
    "images_dir = \"dataset VQA/train2014/train2014\"\n",
    "questions_file = \"dataset VQA/v2_Questions_Train_mscoco/v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    "annotations_file = \"dataset VQA/v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json\"\n",
    "output_pickle_file = \"dataset_v3/vqa_dataset_part.pkl\"  # Modifié pour créer plusieurs fichiers\n",
    "\n",
    "# Charger les données des questions et annotations depuis les fichiers JSON\n",
    "try:\n",
    "    with open(questions_file, 'r', encoding='utf-8') as qfile:\n",
    "        questions_data = json.load(qfile)\n",
    "\n",
    "    with open(annotations_file, 'r', encoding='utf-8') as afile:\n",
    "        annotations_data = json.load(afile)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Erreur de décodage JSON: {e}\")\n",
    "    questions_data = {}\n",
    "    annotations_data = {}\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur est survenue: {e}\")\n",
    "    questions_data = {}\n",
    "    annotations_data = {}\n",
    "\n",
    "# Indexer les questions et annotations par image_id pour un accès rapide\n",
    "questions_index = {}\n",
    "for question in questions_data.get(\"questions\", []):\n",
    "    image_id = question[\"image_id\"]\n",
    "    if image_id not in questions_index:\n",
    "        questions_index[image_id] = []\n",
    "    questions_index[image_id].append(question[\"question\"])\n",
    "\n",
    "annotations_index = {}\n",
    "for annotation in annotations_data.get(\"annotations\", []):\n",
    "    image_id = annotation[\"image_id\"]\n",
    "    if image_id not in annotations_index:\n",
    "        annotations_index[image_id] = []\n",
    "    annotations_index[image_id].append(annotation[\"multiple_choice_answer\"])\n",
    "\n",
    "# Diviser les images en 10 lots\n",
    "total_images = len(questions_index)\n",
    "batch_size = total_images // 10\n",
    "\n",
    "# Fonction pour charger un lot d'images et les sauver dans un fichier pickle\n",
    "def load_and_save_batch(start_idx, end_idx, batch_number):\n",
    "    vqa_dataset_batch = []  # Liste au lieu de dictionnaire\n",
    "    \n",
    "    for idx, (image_id, questions) in enumerate(list(questions_index.items())[start_idx:end_idx], start=start_idx + 1):\n",
    "        # Formater le nom de l'image pour correspondre aux fichiers d'image\n",
    "        formatted_image_id = f\"COCO_train2014_{image_id:012d}\"\n",
    "        image_path = os.path.join(images_dir, f\"{formatted_image_id}.jpg\")\n",
    "        \n",
    "        # Vérifier si l'image existe\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image non trouvée pour l'ID {formatted_image_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Construire les paires question-réponse et les ajouter sous forme de tuple [image_path, question, answer]\n",
    "        for question, answer in zip(questions, annotations_index.get(image_id, [])):\n",
    "            vqa_dataset_batch.append([image_path, question, answer])\n",
    "        \n",
    "        # Afficher le pourcentage de chargement du lot\n",
    "        progress = (idx / total_images) * 100\n",
    "        print(f\"Chargement du lot {batch_number} : {progress:.2f}% ({idx}/{total_images})\")\n",
    "    \n",
    "    # Sauvegarder le lot dans un fichier pickle\n",
    "    with open(output_pickle_file.format(batch_number), 'wb') as pfile:\n",
    "        pickle.dump(vqa_dataset_batch, pfile)\n",
    "        print(f\"Lot {batch_number} sauvegardé dans {output_pickle_file.format(batch_number)}\")\n",
    "\n",
    "# Charger et sauver les 10 lots pour faciliter le traitement\n",
    "for batch_number in range(10):\n",
    "    start_idx = batch_number * batch_size\n",
    "    # Si c'est le dernier lot, on va jusqu'à la fin des données\n",
    "    end_idx = (batch_number + 1) * batch_size if batch_number < 9 else total_images\n",
    "    load_and_save_batch(start_idx, end_idx, batch_number + 1)\n",
    "\n",
    "print(\"Tous les lots ont été traités et sauvegardés.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du dossier contenant les fichiers de lots\n",
    "batch_files_dir = \"dataset_v3\"\n",
    "# Nom des fichiers de lot\n",
    "batch_file_template = \"vqa_dataset_part_{}.pkl\"\n",
    "# Nom du fichier de sortie unique\n",
    "output_file = os.path.join(batch_files_dir, \"vqa_dataset_full.pkl\")\n",
    "\n",
    "# Liste pour stocker tous les lots fusionnés\n",
    "full_dataset = []\n",
    "\n",
    "# Charger chaque fichier de lot et l'ajouter à la liste complète\n",
    "for batch_number in range(10):\n",
    "    batch_file = os.path.join(batch_files_dir, batch_file_template.format(batch_number + 1))\n",
    "    try:\n",
    "        with open(batch_file, 'rb') as pfile:\n",
    "            batch_data = pickle.load(pfile)\n",
    "            full_dataset.extend(batch_data)  # Fusionner les lots en ajoutant chaque élément de batch_data\n",
    "            print(f\"Lot {batch_number + 1} fusionné avec succès.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du lot {batch_number + 1}: {e}\")\n",
    "\n",
    "# Sauvegarder le dataset complet dans un fichier pickle unique (fusion)\n",
    "with open(output_file, 'wb') as outfile:\n",
    "    pickle.dump(full_dataset, outfile)\n",
    "    print(f\"Dataset complet sauvegardé dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae99a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Préparation données validation en un seul fichier pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir les chemins vers les images et les fichiers JSON\n",
    "images_dir = \"dataset VQA/val2014/val2014\"\n",
    "questions_file = \"dataset VQA/v2_Questions_Val_mscoco/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "annotations_file = \"dataset VQA/v2_Annotations_Val_mscoco/v2_mscoco_val2014_annotations.json\"\n",
    "output_pickle_file = \"dataset_v3/vqa_dataset_val_part_{}.pkl\"  \n",
    "\n",
    "# Charger les données des questions et annotations depuis les fichiers JSON\n",
    "try:\n",
    "    with open(questions_file, 'r', encoding='utf-8') as qfile:\n",
    "        questions_data = json.load(qfile)\n",
    "\n",
    "    with open(annotations_file, 'r', encoding='utf-8') as afile:\n",
    "        annotations_data = json.load(afile)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Erreur de décodage JSON: {e}\")\n",
    "    questions_data = {}\n",
    "    annotations_data = {}\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur est survenue: {e}\")\n",
    "    questions_data = {}\n",
    "    annotations_data = {}\n",
    "\n",
    "# Indexer les questions et annotations par image_id pour un accès rapide\n",
    "questions_index = {}\n",
    "for question in questions_data.get(\"questions\", []):\n",
    "    image_id = question[\"image_id\"]\n",
    "    if image_id not in questions_index:\n",
    "        questions_index[image_id] = []\n",
    "    questions_index[image_id].append(question[\"question\"])\n",
    "\n",
    "annotations_index = {}\n",
    "for annotation in annotations_data.get(\"annotations\", []):\n",
    "    image_id = annotation[\"image_id\"]\n",
    "    if image_id not in annotations_index:\n",
    "        annotations_index[image_id] = []\n",
    "    annotations_index[image_id].append(annotation[\"multiple_choice_answer\"])\n",
    "\n",
    "# Diviser les images en 10 lots pour faciliter le traitement\n",
    "total_images = len(questions_index)\n",
    "batch_size = total_images // 10\n",
    "\n",
    "# Fonction pour charger un lot d'images et les sauver dans un fichier pickle\n",
    "def load_and_save_batch(start_idx, end_idx, batch_number):\n",
    "    vqa_dataset_batch = []  # Liste au lieu de dictionnaire\n",
    "    \n",
    "    for idx, (image_id, questions) in enumerate(list(questions_index.items())[start_idx:end_idx], start=start_idx + 1):\n",
    "        # Formater le nom de l'image pour correspondre aux fichiers d'image\n",
    "        formatted_image_id = f\"COCO_val2014_{image_id:012d}\"\n",
    "        image_path = os.path.join(images_dir, f\"{formatted_image_id}.jpg\")\n",
    "        \n",
    "        # Vérifier si l'image existe\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image non trouvée pour l'ID {formatted_image_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Construire les paires question-réponse et les ajouter sous forme de tuple [image_path, question, answer]\n",
    "        for question, answer in zip(questions, annotations_index.get(image_id, [])):\n",
    "            vqa_dataset_batch.append([image_path, question, answer])\n",
    "        \n",
    "        # Afficher le pourcentage de chargement du lot\n",
    "        progress = (idx / total_images) * 100\n",
    "        print(f\"Chargement du lot {batch_number} : {progress:.2f}% ({idx}/{total_images})\")\n",
    "    \n",
    "    # Sauvegarder le lot dans un fichier pickle\n",
    "    with open(output_pickle_file.format(batch_number), 'wb') as pfile:\n",
    "        pickle.dump(vqa_dataset_batch, pfile)\n",
    "        print(f\"Lot {batch_number} sauvegardé dans {output_pickle_file.format(batch_number)}\")\n",
    "\n",
    "# Charger et sauver les 10 lots\n",
    "for batch_number in range(10):\n",
    "    start_idx = batch_number * batch_size\n",
    "    # Si c'est le dernier lot, on va jusqu'à la fin des données\n",
    "    end_idx = (batch_number + 1) * batch_size if batch_number < 9 else total_images\n",
    "    load_and_save_batch(start_idx, end_idx, batch_number + 1)\n",
    "\n",
    "print(\"Tous les lots ont été traités et sauvegardés.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae531d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du dossier contenant les fichiers de lots\n",
    "batch_files_dir = \"dataset_v3\"\n",
    "# Nom des fichiers de lot\n",
    "batch_file_template = \"vqa_dataset_val_part_{}.pkl\"\n",
    "# Nom du fichier de sortie unique\n",
    "output_file = os.path.join(batch_files_dir, \"vqa_dataset_val_full.pkl\")\n",
    "\n",
    "# Liste pour stocker tous les lots fusionnés\n",
    "full_dataset = []\n",
    "\n",
    "# Charger chaque fichier de lot et l'ajouter à la liste complète\n",
    "for batch_number in range(10):\n",
    "    batch_file = os.path.join(batch_files_dir, batch_file_template.format(batch_number + 1))\n",
    "    try:\n",
    "        with open(batch_file, 'rb') as pfile:\n",
    "            batch_data = pickle.load(pfile)\n",
    "            full_dataset.extend(batch_data)  # Fusionner les lots en ajoutant chaque élément de batch_data\n",
    "            print(f\"Lot {batch_number + 1} fusionné avec succès.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du lot {batch_number + 1}: {e}\")\n",
    "\n",
    "# Sauvegarder le dataset complet dans un fichier pickle unique\n",
    "with open(output_file, 'wb') as outfile:\n",
    "    pickle.dump(full_dataset, outfile)\n",
    "    print(f\"Dataset complet sauvegardé dans {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
